\documentclass[letterpaper,11pt]{article}

\usepackage{amsmath, amsfonts, amsthm, graphics, latexsym, multicol}
\usepackage{mathtools}
\usepackage{xcolor}
\pagestyle{empty}
\usepackage{graphicx,float}
\usepackage{enumitem}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{mcode}
\usepackage{subcaption}

\begin{document}

\title{Project 2: Neural Networks \\
		\large MAT128B Winter 2020}
\author{Eli , Frances, Nikos Trembois}
\date{March 6, 2020}
\maketitle
\tableofcontents
\newpage

\section*{Introduction}
In this project, we will train a neural network to read and correctly output an image. The structure of our network is the number of layers (input, output, and hidden layers) each of which we'll assign a number of neurons. We'll start with the function readMNIST which reads the digits and labels from the MNIST data files. This will be our training set. This function returns a $20 x 20$ matrix for each image, so $400$ pixels represented as neurons, each assigned n input connections and n weights. This set of neurons will be our input layer. Through the forward passing and backpropagation of the values and weights of each training pair, we will train the network to adjust its weights between each layer, in order to minimize the error between the input and target values.

\section{Plot Digits}

\section{A Neuron}
For $0$ NET, the OUT value is 0.5. As NET increases, OUT converges to $1$.
We can use other functions s.t. the initial growth is exponential, and the total count has an upper limit.
NET = $\displaystyle{\sum^{n}_{i=1}O_{i}W_{i}}$.
Verify F'(NET) = $OUT(1-OUT)$, where OUT = $F(NET) = \frac{1}{1 + e^{-NET}}$.

\section{Multilayer Network}
Our output layer will be an array of 10 neurons...
Backpropagation: Talk abt deltas/ difference in calculating deltas of output vs. hidden
\section{Initializing the network}
Assign random small number to each weight...
\section{Training the Network}
To see how accurately we've trained our network, we'll evaluate the values of error, avgError, prediction, correctness, and avgCorrectness. Our TestNetwork function will run the ForwardPass on our network with the final adjusted weights, WFinal.

\section{Dependence on Parameters}
Since little is known about how the network chooses its weights during the training process, selecting the ideal number of network layers and neurons, as well as initial weights and training rate is a difficult choice. Instead, a parameter study is used to find the dependence on specifc variables. Training a network takes time and studying more variations increases the likelihood of finding a better network. First a parameter study was done for a few networks to see their dependence on the number of training images. The effect of the training images is trivial, but a value for which the sets reach a plateau was sought. This turned out to be about one-fifth of the training set data. So the rest of the parameter study was carried out using one-fifth of the training set so more configurations could be studied in less time. Additionally, the effects of the extremes for the training rate and initial weights were studied, then a subrange was chosen for subsequent studies. 
% Need to recall these values
% We found that training rates greater than or less than ... had poor results, while training rates in the range of ?? to ?? gave the best results with little variation within the range. Similary for weights ...
Simultaneously, the effect of the number of layers and neurons in the network were studied. The number of hidden layers ranged from one to three and selection of the number of neurons in the layers was mostly random choice. The single hidden layer networks appeared to perform the best with increasing number of neurons given better results.


\section{Conclusion}

\newpage

\section{Appendix}

\subsection{Code}
%\lstinputlisting[breaklines=true]{../Code/Project2.m}

\subsection{Group Work}

\end{document}
