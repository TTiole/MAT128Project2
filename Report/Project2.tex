\documentclass[letterpaper,11pt]{article}

\usepackage{amsmath, amsfonts, amsthm, graphics, latexsym, multicol}
\usepackage{mathtools}
\usepackage{xcolor}
\pagestyle{empty}
\usepackage{graphicx,float}
\usepackage{enumitem}
\usepackage[ruled,vlined]{algorithm2e}
\usepackage{mcode}
\usepackage{subcaption}

\begin{document}

\title{Project 2: Neural Networks \\
		\large MAT128B Winter 2020}
\author{Eli , Frances, Nikos Trembois}
\date{March 6, 2020}
\maketitle
\tableofcontents
\newpage

\section*{Introduction}
In this project, we will train a neural network to read and correctly output an image. The structure of our network is the number of layers (input, output, and hidden layers) each of which we'll assign a number of neurons. We'll start with the function readMNIST which reads the digits and labels from the MNIST data files. This will be our training set. This function returns a $20 x 20$ matrix for each image, so $400$ pixels represented as neurons, each assigned n input connections and n weights. This set of neurons will be our input layer. Through the forward passing and backpropagation of the values and weights of each training pair, we will train the network to adjust its weights between each layer, in order to minimize the error between the input and target values.

\section{Plot Digits}

\begin{figure}
\centering
	\begin{subfigure}[b]{0.49\linewidth}
		\includegraphics[width=\linewidth]{../Figures/digit1.png}
	\end{subfigure}
	\begin{subfigure}[b]{0.49\linewidth}
		\includegraphics[width=\linewidth]{../Figures/digit2.png}
	\end{subfigure}
\caption{Handwritten digits represented by pixels}
\end{figure}

\section{A Neuron} \label{ref:neuron}
For $0$ NET, the OUT value is 0.5. As NET increases, OUT converges to $1$.
We can use other functions s.t. the initial growth is exponential, and the total count has an upper limit.
NET = $\displaystyle{\sum^{n}_{i=1}O_{i}W_{i}}$.
Verify F'(NET) = $OUT(1-OUT)$, where OUT = $F(NET) = \frac{1}{1 + e^{-NET}}$.

\section{Multilayer Network}
Our network consists of an input layer, an output layer, and any number of hidden layers. The number of input neurons is given by the data we are training and testing. Since the input data is a 20 by 20 pixel image, the input layer consists of 400 neurons. Likewise, the output layer is determined by the values we wish to predict. Since we are reading handwritten digits, there are ten possible outcomes, which are the numbers 0-9. On the other hand the hidden layers have a lot of flexibility and their configuration are only limited by their performance which will be discussed in section \ref{sec:param}: Dependence on Parameters. We can alter both the number of hidden layers and the number of neurons in each layer. Once a hidden layer configuration is chosen, the network can be trained and tested.

The training process involves a forward and backward pass. During the forward pass, the neuron values are calculated as described in section \ref{sec:neuron}. During the backward pass, errors are calculated and propagated backwards through the network and used to update the values of the weights. Calculating the error at the output is simple, the program checks the expected value with the actual value. In our network we desire the output to be one for the neuron respresenting the actual digit and zero for any others. The output, in a way, respresents the networks certainty in predicting each digit, where 1 is 100\% confidence. The weights of the network connecting the inputs to a given output should be changed in proportion to the error as large errors will require a greater change in weights to correct. However, the weights cannot be corrected by just the error alone. The weights connecting the last hidden layer to the output layer are adjusted using the neurons values as well. The method for updating the weights is given in equations \ref{eq:delta} and \ref{eq:deltaW}. What these equations show is how the value of the error is used to update the value of the weights. A single $\delta$ value is given for each neuron. 

\begin{align}
\delta &= OUT_{q,k}(1-OUT_{q,k})(ERROR) \\
\Delta w_{pq,k} = \eta \delta_{q,k} OUT_{p,j}
\end{align}

Backpropagation: Talk abt deltas/ difference in calculating deltas of output vs. hidden
\section{Initializing the network}
Assign random small number to each weight...
\section{Training the Network}
To see how accurately we've trained our network, we'll evaluate the values of error, avgError, prediction, correctness, and avgCorrectness. Our TestNetwork function will run the ForwardPass on our network with the final adjusted weights, WFinal.

\section{Dependence on Parameters} \label{sec:param}
Since little is known about how the network chooses its weights during the training process, selecting the ideal number of network layers and neurons, as well as initial weights and training rate is a difficult choice. Instead, a parameter study is used to find the dependence on specifc variables. Training a network takes time and studying more variations increases the likelihood of finding a better network. First a parameter study was done for a few networks to see their dependence on the number of training images. The effect of the training images is trivial, but a value for which the sets reach a plateau was sought. This turned out to be about one-fifth of the training set data. So the rest of the parameter study was carried out using one-fifth of the training set so more configurations could be studied in less time. Additionally, the effects of the extremes for the training rate and initial weights were studied, then a subrange was chosen for subsequent studies. 
% Need to recall these values
% We found that training rates greater than or less than ... had poor results, while training rates in the range of ?? to ?? gave the best results with little variation within the range. Similary for weights ...
Simultaneously, the effect of the number of layers and neurons in the network were studied. The number of hidden layers ranged from one to three and selection of the number of neurons in the layers was mostly random choice. The single hidden layer networks appeared to perform the best with increasing number of neurons given better results.


\section{Conclusion}

\newpage

\section{Appendix}

\subsection{Code}
%\lstinputlisting[breaklines=true]{../Code/Project2.m}

\subsection{Group Work}

\end{document}
